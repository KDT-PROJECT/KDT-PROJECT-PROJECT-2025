#!/usr/bin/env python3
"""
í‰ê°€ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
TASK-016: í‰ê°€ ìŠ¤ìœ„íŠ¸ - ì •í™•ë„/ê°ì£¼ìœ¨/ì§€ì—° ë²¤ì¹˜
"""

import argparse
import json
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pipelines.eval_suite import EvaluationSuite
from utils.sql_text2sql import TextToSQL
from utils.database import DatabaseManager
from utils.hybrid_retrieval import HybridRetrieval
from utils.report import ReportGenerator
from infrastructure.logging_service import LoggingService


def setup_components(config: dict) -> tuple:
    """í‰ê°€ì— í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    logger = LoggingService().get_system_logger()
    
    try:
        # Database manager ì„¤ì •
        db_manager = DatabaseManager(config.get("database", {}))
        
        # Text-to-SQL ì—”ì§„ ì„¤ì •
        sql_engine = TextToSQL(
            db_manager=db_manager,
            model_name=config.get("llm", {}).get("model_name", "microsoft/DialoGPT-medium"),
            device=config.get("llm", {}).get("device", "cpu")
        )
        
        # RAG ì—”ì§„ ì„¤ì • (ì‹¤ì œ êµ¬í˜„ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        rag_engine = None
        if config.get("enable_rag_evaluation", True):
            try:
                rag_engine = HybridRetrieval(
                    vector_store_path=config.get("vector_store", {}).get("path", "data/vector_store"),
                    bm25_path=config.get("bm25", {}).get("path", "data/bm25_index")
                )
            except Exception as e:
                logger.warning(f"RAG ì—”ì§„ ì„¤ì • ì‹¤íŒ¨: {e}")
                rag_engine = None
        
        # ë³´ê³ ì„œ ìƒì„±ê¸° ì„¤ì •
        report_generator = None
        if config.get("enable_report_evaluation", True):
            try:
                report_generator = ReportGenerator(
                    sql_engine=sql_engine,
                    rag_engine=rag_engine,
                    config=config.get("report", {})
                )
            except Exception as e:
                logger.warning(f"ë³´ê³ ì„œ ìƒì„±ê¸° ì„¤ì • ì‹¤íŒ¨: {e}")
                report_generator = None
        
        return sql_engine, rag_engine, report_generator
        
    except Exception as e:
        logger.error(f"ì»´í¬ë„ŒíŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        raise


def load_config(config_path: str) -> dict:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        return get_default_config()
    except json.JSONDecodeError as e:
        print(f"ì„¤ì • íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return get_default_config()


def get_default_config() -> dict:
    """ê¸°ë³¸ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        "database": {
            "host": "localhost",
            "port": 3306,
            "user": "root",
            "password": "password",
            "database": "seoul_commercial"
        },
        "llm": {
            "model_name": "microsoft/DialoGPT-medium",
            "device": "cpu"
        },
        "vector_store": {
            "path": "data/vector_store"
        },
        "bm25": {
            "path": "data/bm25_index"
        },
        "report": {
            "template_path": "prompts/report_templates"
        },
        "evaluation": {
            "results_path": "models/artifacts/evaluation"
        },
        "enable_rag_evaluation": True,
        "enable_report_evaluation": True
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì„œìš¸ ìƒê¶Œ ë¶„ì„ LLM ì‹œìŠ¤í…œ í‰ê°€ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰")
    parser.add_argument(
        "--config", 
        type=str, 
        default="config/evaluation_config.json",
        help="í‰ê°€ ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--components",
        nargs="+",
        choices=["sql", "rag", "report", "all"],
        default=["all"],
        help="í‰ê°€í•  ì»´í¬ë„ŒíŠ¸ ì„ íƒ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="evaluation_report.md",
        help="í‰ê°€ ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    
    # ë¡œê¹… ì„¤ì •
    logger = LoggingService().get_system_logger()
    if args.verbose:
        logger.setLevel("DEBUG")
    
    try:
        print("ğŸš€ ì„œìš¸ ìƒê¶Œ ë¶„ì„ LLM ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘")
        print(f"ğŸ“‹ í‰ê°€ ì»´í¬ë„ŒíŠ¸: {', '.join(args.components)}")
        print(f"ğŸ“ ì„¤ì • íŒŒì¼: {args.config}")
        print(f"ğŸ“„ ì¶œë ¥ íŒŒì¼: {args.output}")
        print("-" * 50)
        
        # í‰ê°€ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”
        eval_config = config.get("evaluation", {})
        eval_suite = EvaluationSuite(eval_config)
        
        # ì»´í¬ë„ŒíŠ¸ ì„¤ì •
        sql_engine = None
        rag_engine = None
        report_generator = None
        
        if "all" in args.components or "sql" in args.components:
            print("ğŸ”§ Text-to-SQL ì—”ì§„ ì„¤ì • ì¤‘...")
            sql_engine, _, _ = setup_components(config)
            print("âœ… Text-to-SQL ì—”ì§„ ì„¤ì • ì™„ë£Œ")
        
        if "all" in args.components or "rag" in args.components:
            print("ğŸ”§ RAG ì—”ì§„ ì„¤ì • ì¤‘...")
            _, rag_engine, _ = setup_components(config)
            if rag_engine:
                print("âœ… RAG ì—”ì§„ ì„¤ì • ì™„ë£Œ")
            else:
                print("âš ï¸  RAG ì—”ì§„ ì„¤ì • ê±´ë„ˆëœ€")
        
        if "all" in args.components or "report" in args.components:
            print("ğŸ”§ ë³´ê³ ì„œ ìƒì„±ê¸° ì„¤ì • ì¤‘...")
            _, _, report_generator = setup_components(config)
            if report_generator:
                print("âœ… ë³´ê³ ì„œ ìƒì„±ê¸° ì„¤ì • ì™„ë£Œ")
            else:
                print("âš ï¸  ë³´ê³ ì„œ ìƒì„±ê¸° ì„¤ì • ê±´ë„ˆëœ€")
        
        # í‰ê°€ ì‹¤í–‰
        print("\nğŸ“Š í‰ê°€ ì‹¤í–‰ ì¤‘...")
        results = eval_suite.run_comprehensive_evaluation(
            sql_engine=sql_engine,
            rag_engine=rag_engine,
            report_generator=report_generator
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“ˆ í‰ê°€ ê²°ê³¼:")
        print(f"â±ï¸  ì´ í‰ê°€ ì‹œê°„: {results.get('total_evaluation_time', 0):.2f}ì´ˆ")
        
        # KPI ì„±ê³¼ ì¶œë ¥
        kpi_perf = results.get("overall_metrics", {}).get("kpi_performance", {})
        if kpi_perf:
            print("\nğŸ¯ KPI ì„±ê³¼:")
            print(f"  â€¢ Text-to-SQL ì •í™•ë„: {kpi_perf.get('sql_accuracy_achieved', 0):.1%} "
                  f"({kpi_perf.get('sql_accuracy_status', 'N/A')})")
            print(f"  â€¢ ì‘ë‹µ ì‹œê°„ (P95): {kpi_perf.get('response_time_achieved', 0):.2f}ì´ˆ "
                  f"({kpi_perf.get('response_time_status', 'N/A')})")
            print(f"  â€¢ ê·¼ê±° ê°ì£¼ í¬í•¨ë¥ : {kpi_perf.get('evidence_citation_achieved', 0):.1%} "
                  f"({kpi_perf.get('evidence_citation_status', 'N/A')})")
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        components = results.get("components", {})
        if "text_to_sql" in components:
            sql_data = components["text_to_sql"]
            print(f"\nğŸ” Text-to-SQL ìƒì„¸:")
            print(f"  â€¢ ì •í™•ë„: {sql_data.get('accuracy', 0):.1%}")
            print(f"  â€¢ ì„±ê³µí•œ ì¿¼ë¦¬: {sql_data.get('successful_queries', 0)}/{sql_data.get('total_queries', 0)}")
            print(f"  â€¢ í‰ê·  ì‘ë‹µ ì‹œê°„: {sql_data.get('avg_response_time', 0):.2f}ì´ˆ")
        
        if "rag" in components:
            rag_data = components["rag"]
            print(f"\nğŸ” RAG ìƒì„¸:")
            print(f"  â€¢ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {rag_data.get('avg_relevance_score', 0):.3f}")
            print(f"  â€¢ í‰ê·  ì‘ë‹µ ì‹œê°„: {rag_data.get('avg_response_time', 0):.2f}ì´ˆ")
        
        if "report_generation" in components:
            report_data = components["report_generation"]
            print(f"\nğŸ” ë³´ê³ ì„œ ìƒì„± ìƒì„¸:")
            print(f"  â€¢ ì„±ê³µë¥ : {report_data.get('success_rate', 0):.1%}")
            print(f"  â€¢ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {report_data.get('avg_quality_score', 0):.3f}")
            print(f"  â€¢ í‰ê·  ìƒì„± ì‹œê°„: {report_data.get('avg_generation_time', 0):.2f}ì´ˆ")
        
        # ë³´ê³ ì„œ ìƒì„±
        print(f"\nğŸ“ í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì¤‘: {args.output}")
        report_content = eval_suite.generate_evaluation_report(results)
        
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print("âœ… í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {args.output}")
        
        # JSON ê²°ê³¼ë„ ì €ì¥
        json_output = args.output.replace('.md', '.json')
        with open(json_output, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š JSON ê²°ê³¼: {json_output}")
        
    except Exception as e:
        logger.error(f"í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
